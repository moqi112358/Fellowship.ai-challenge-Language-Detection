{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract sentence data from raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we have 21 European languages: Romanic (French, Italian, Spanish, Portuguese, Romanian), Germanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian, Czech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian), Baltic (Latvian, Lithuanian), and Greek. And we need to build a language detection engines to identify different languages of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the raw data, we have 21 folders for the 21 European languages, and each folder contains thousands of text files. All formats contain document ( &lt; CHAPTER id &gt;), speaker (&lt; SPEAKER id name language &gt;), and paragraph (&lt; P &gt;) mark-up on a separate line. Some special HTML entities and noisy characters are not removed from the data. In this part, we need to extract sentence data from the raw data. Here are some keys in this step:\n",
    "\n",
    "#### 1. Strip empty lines and their correspondences \n",
    "#### 2. Remove lines with XML-Tags (starting with \"<\") \n",
    "#### 3. Lowercase the text\n",
    "#### 4. Split the text data to sentences by period\n",
    "#### 5. Remove noisy characters and punctuation\n",
    "#### 6. Transform long space to single space\n",
    "#### 7. Control the length of each sentence ( > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the abbreviation labels for the 21 European languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "labels = ['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', \n",
    "          'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', \n",
    "          'pt', 'ro', 'sk', 'sl', 'sv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two functions to  extract clean text from all files of one language. We split all paragraph to sentences and store all the sentences in one set to make sure there is no duplicate sentence. In the `extract_clean_sentence` function, we use codecs package to open the text file instead of built-in function because there are some noisy characters e.g. byte 0x95 and 'utf-8' codec can't decode byte 0x95, thus we can ignore these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_sentence(file_path, language):\n",
    "    '''\n",
    "    Goal:  extract clean text from all files of one language. We split all paragraph to sentences and store all the sentences (we set a threshold for the sentence length: length > 1) in one set.\n",
    "    @param txt_file_list: a string which is a language\n",
    "           file_path: the file path contains all language folders.\n",
    "    @return clean_sentences:  a set of strings (sentences).\n",
    "    '''\n",
    "    clean_sentences = set() # use set to make sure there is no duplicate sentence\n",
    "    file_list = os.listdir(file_path+language+'\\\\')\n",
    "    for file in file_list:\n",
    "        # Here we use codecs package to open the text file because there are some noisy characters e.g. byte 0x95\n",
    "        # and 'utf-8' codec can't decode byte 0x95, thus we can ignore these errors\n",
    "        with codecs.open(file_path+language+'\\\\'+file, 'r', encoding=\"utf-8\", errors = 'ignore') as f:\n",
    "            for line in f:\n",
    "                line = line.strip() # 1. Strip empty lines\n",
    "                if not line or line.startswith(\"<\"):  # 2.Skip XML-Tags e.g. <CHAPTER ID=\"006-01\">\n",
    "                    continue\n",
    "                line = line.split('.') # 4. split the text data to sentences by period\n",
    "                for sentence in line:\n",
    "                    sentence = sentence.strip()\n",
    "                    sentence = clean_sentence(sentence)\n",
    "                    if len(sentence) > 1: # 7. control the length of sentence\n",
    "                        clean_sentences.add(sentence)\n",
    "    f = open(language + '_sentence.pkl','wb')\n",
    "    pickle.dump(clean_sentences, f)\n",
    "    f.close()\n",
    "    return clean_sentences\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    '''\n",
    "    Goal: remove punctuation in the sentence\n",
    "    @param sentence: a string which contains punctuation\n",
    "    @return clean_sentences:  a strings (sentence).\n",
    "    '''\n",
    "    output = \"\".join(re.findall(\"[^\\t\\d\\r\\n–{}]+\".format(string.punctuation),sentence.lower())) # 3,5. remove punctuation and lowercase the text¶\n",
    "    output = output.strip()\n",
    "    output = re.sub(\"\\s+\", \" \", output) # 6. transform long space to ' '\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 21 European languages, we store the senteces for each language in the language_sentence.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish bg\n",
      "Finish cs\n",
      "Finish da\n",
      "Finish de\n",
      "Finish el\n",
      "Finish en\n",
      "Finish es\n",
      "Finish et\n",
      "Finish fi\n",
      "Finish fr\n",
      "Finish hu\n",
      "Finish it\n",
      "Finish lt\n",
      "Finish lv\n",
      "Finish nl\n",
      "Finish pl\n",
      "Finish pt\n",
      "Finish ro\n",
      "Finish sk\n",
      "Finish sl\n",
      "Finish sv\n"
     ]
    }
   ],
   "source": [
    "for i in labels:\n",
    "    f = open(i + '_sentence.pkl','wb')\n",
    "    tmp = extract_clean_sentence('../txt/', i)\n",
    "    pickle.dump(tmp, f)\n",
    "    f.close()\n",
    "    print('Finish', i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
